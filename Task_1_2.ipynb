{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_1_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Li1_OSXkASS",
        "outputId": "512e67fa-9032-4aad-9f0d-0edc40bd5bf9"
      },
      "source": [
        "#Importing all relevant packages \r\n",
        "import gym\r\n",
        "from IPython.display import clear_output\r\n",
        "from time import sleep\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "from IPython.display import clear_output\r\n",
        "\r\n",
        "#Using the Taxi-v3 game in gym and storing in variable env\r\n",
        "env = gym.make(\"Taxi-v3\").env\r\n",
        "\r\n",
        "#Providing a render or state instance, resetting again and giving another state instance\r\n",
        "#We use this to show the use of reset, render functions and step sample \r\n",
        "env.render()\r\n",
        "env.reset() # reset environment to a new, random state\r\n",
        "env.render()\r\n",
        "\r\n",
        "#To show the Action and the State Spaces\r\n",
        "print(\"Action Space {}\".format(env.action_space))\r\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : :\u001b[43m \u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "+---------+\n",
            "|R: |\u001b[43m \u001b[0m: :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GkiT1Oho89Q",
        "outputId": "6879057e-453c-4d11-9bdb-af897ca92ddb"
      },
      "source": [
        "#We start by providing a specific state. In our case it is state 428 and we fix this for further problem\r\n",
        "#The position in yellow shows the current state of the vehicle (428) and we aim to reach Destination R first which\r\n",
        "#is our first goal \r\n",
        "state = env.encode(4, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\r\n",
        "print(\"State:\", state)\r\n",
        "\r\n",
        "env.s = state\r\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State: 428\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Arj-PboeqXsj",
        "outputId": "0f2ad4a4-d9bd-49ca-914c-d76acece957b"
      },
      "source": [
        "#Here we see the possible actions at state 428. 0 is south, 1 is north, 2 is east, 3 is west, 4 is pickup and 5 is dropoff\r\n",
        "#We see the possible states the agent can move to and corresponding rewards (-1 for direction movement and -10 for pickup/dropoff)\r\n",
        "#We see the rewards issued for each action below\r\n",
        " \r\n",
        "env.P[428]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 328, -1, False)],\n",
              " 2: [(1.0, 448, -1, False)],\n",
              " 3: [(1.0, 428, -1, False)],\n",
              " 4: [(1.0, 428, -10, False)],\n",
              " 5: [(1.0, 428, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "lAn4w-pyrO3x",
        "outputId": "8e5d1851-3d24-4746-d42a-8bd2eb8cb6c6"
      },
      "source": [
        "#CASE 1 - WITH RANDOM POLICY (NO REINFORCEMENT LEARNING APPLIED)\r\n",
        "#EVALUATION OF ACTIONS\r\n",
        "# We clearly see that the timesteps taken and penalties incurred are extremely high\r\n",
        "# This is not the right policy for this problem \r\n",
        "env.s = 428  # set environment to illustration's state\r\n",
        "\r\n",
        "epochs = 0\r\n",
        "penalties, reward = 0, 0\r\n",
        "\r\n",
        "frames = [] # for animation\r\n",
        "\r\n",
        "done = False\r\n",
        "\r\n",
        "while not done:\r\n",
        "    action = env.action_space.sample()\r\n",
        "    state, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "    if reward == -10:\r\n",
        "        penalties += 1\r\n",
        "    \r\n",
        "    # Put each rendered frame into dict for animation\r\n",
        "    frames.append({\r\n",
        "        'frame': env.render(mode='ansi'),\r\n",
        "        'state': state,\r\n",
        "        'action': action,\r\n",
        "        'reward': reward\r\n",
        "        }\r\n",
        "    )\r\n",
        "\r\n",
        "    epochs += 1\r\n",
        "    \r\n",
        "    \r\n",
        "print(\"Timesteps taken: {}\".format(epochs))\r\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timesteps taken: 841\n",
            "Penalties incurred: 245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SGJPBEwmtQQD",
        "outputId": "1ee1cf3c-e72b-444a-a001-3d5d254b8a99"
      },
      "source": [
        "#CASE 1 - WITH RANDOM POLICY (NO REINFORCEMENT LEARNING APPLIED)\r\n",
        "#Illustration showing the state shift of the agent for this random policy\r\n",
        "#Instead of showing a video capture as per the lectures, we show frames of each state shift back to back\r\n",
        "\r\n",
        "def print_frames(frames):\r\n",
        "    for i, frame in enumerate(frames):\r\n",
        "        clear_output(wait=True)\r\n",
        "        print(frame['frame'])\r\n",
        "        print(f\"Timestep: {i + 1}\")\r\n",
        "        print(f\"State: {frame['state']}\")\r\n",
        "        print(f\"Action: {frame['action']}\")\r\n",
        "        print(f\"Reward: {frame['reward']}\")\r\n",
        "        sleep(.05)\r\n",
        "        \r\n",
        "print_frames(frames)"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 841\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "F5bjUdtitrzy",
        "outputId": "77ca5830-b570-475a-c862-14fc8ca83c36"
      },
      "source": [
        "#CASE 2 - WITH DEFINED POLICY (Q-LEARNING APPLIED)\r\n",
        "#EVALUATION OF ACTIONS\r\n",
        "#Resetting the Q-table to all zeros. We start Q-learning policy here onwards\r\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\r\n",
        "\r\n",
        "#Training the agent\r\n",
        "#Storing Hyperparameters values. We do not tune them here since we focus on the Policy Definition Aspect.\r\n",
        "#We ensure that we apply a Policy that gives us a better approach than the random policy \r\n",
        "alpha = 0.1\r\n",
        "gamma = 0.6\r\n",
        "epsilon = 0.1\r\n",
        "\r\n",
        "# For plotting metrics\r\n",
        "all_epochs = []\r\n",
        "all_penalties = []\r\n",
        "\r\n",
        "#Exploration vs Exploitation\r\n",
        "#Choosing randon action vs choosing action based on already learnt Q-values\r\n",
        "# Iterations kept to 10,000. We can vary this. \r\n",
        "for i in range(1, 10001):\r\n",
        "    state = env.reset()\r\n",
        "\r\n",
        "    epochs, penalties, reward, = 0, 0, 0\r\n",
        "    done = False\r\n",
        "    \r\n",
        "    while not done:\r\n",
        "        if random.uniform(0, 1) < epsilon:\r\n",
        "            action = env.action_space.sample() # Explore action space\r\n",
        "        else:\r\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values\r\n",
        "\r\n",
        "        next_state, reward, done, info = env.step(action) \r\n",
        "        \r\n",
        "        old_value = q_table[state, action]\r\n",
        "        next_max = np.max(q_table[next_state])\r\n",
        "        \r\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\r\n",
        "        q_table[state, action] = new_value\r\n",
        "        if reward == -10:\r\n",
        "            penalties += 1\r\n",
        "\r\n",
        "        state = next_state\r\n",
        "        epochs += 1\r\n",
        "        \r\n",
        "    if i % 100 == 0:\r\n",
        "        clear_output(wait=True)\r\n",
        "        print(f\"Episode: {i}\")\r\n",
        "\r\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 10000\n",
            "Training finished.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OwQRfVOvuFbZ",
        "outputId": "3b8bc697-a64e-4393-9d30-18b43fb3448e"
      },
      "source": [
        "#Revised Q values for state 428. We will use the highest reward action according to this to reach the next step \r\n",
        "q_table[428]"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.37215359, -2.36390142, -2.37082892, -2.37689387, -4.48600264,\n",
              "       -6.15881733])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "clUVG_edv0YL",
        "outputId": "17bb2241-85d2-495d-a8f3-dad8ae06efc4"
      },
      "source": [
        "#Evaluation of agent's performance after Q-learning\r\n",
        "#We Clearly see that average timesteps per episode has drastically reduced and the penalty is 0\r\n",
        "#Much better Evaluation Results after we apply a well defined policy  \r\n",
        "\r\n",
        "total_epochs, total_penalties = 0, 0\r\n",
        "episodes = 10\r\n",
        "\r\n",
        "for _ in range(episodes):\r\n",
        "    state = env.reset()\r\n",
        "    epochs, penalties, reward = 0, 0, 0\r\n",
        "    \r\n",
        "    done = False\r\n",
        "    \r\n",
        "    while not done:\r\n",
        "        action = np.argmax(q_table[state])\r\n",
        "        state, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "        if reward == -10:\r\n",
        "            penalties += 1\r\n",
        "\r\n",
        "        epochs += 1\r\n",
        "\r\n",
        "    total_penalties += penalties\r\n",
        "    total_epochs += epochs\r\n",
        "\r\n",
        "print(f\"Results after {episodes} episodes:\")\r\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\r\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 10 episodes:\n",
            "Average timesteps per episode: 12.3\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}